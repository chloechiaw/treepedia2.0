{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "import cv2 \n",
    "# on chloe's local computer, to turn do conda activate py36 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import os\n",
    "from skimage.io import imread\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "# from albumentations import HorizontalFlip, VerticalFlip, RandomRotate90, Normalize\n",
    "# from albumentations import OneOf, Compose\n",
    "# from collections import OrderedDict\n",
    "# from ever.interface import ConfigurableMixin\n",
    "# from torch.utils.data import SequentialSampler, RandomSampler\n",
    "# from ever.api.data import CrossValSamplerGenerator\n",
    "# # import numpy as np\n",
    "# # import logging\n",
    "# # from utils.tools import seed_worker\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# LABEL_MAP = OrderedDict(\n",
    "#     Background=0,\n",
    "#     Building=1,\n",
    "#     Road=2,\n",
    "# #     Water=3,\n",
    "# #     Barren=4,\n",
    "# #     Forest=5,\n",
    "# #     Agricultural=6\n",
    "# # )\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "COLOR_MAP = dict(\n",
    "    IGNORE=(0, 0, 0),\n",
    "    Background=(255, 255, 255),\n",
    "    Building=(255, 0, 0),\n",
    "    Road=(255, 255, 0),\n",
    "    Water=(0, 0, 255),\n",
    "    Barren=(159, 129, 183),\n",
    "    Forest=(0, 255, 0),\n",
    "    Agricultural=(255, 195, 128),\n",
    ")\n",
    "\n",
    "# def render(mask_path, vis_path):\n",
    "#     segmentation_map = np.array(Image.open(mask_path)).astype(np.uint8) # returns class values for each pixel \n",
    "#     cm = np.array(list(COLOR_MAP.values())).astype(np.uint8) # returns color map values that correspond to the class values per pixel \n",
    "#     color_img = cm[segmentation_map]\n",
    "#     color_img = Image.fromarray(np.uint8(color_img))\n",
    "#     color_img.save(vis_path)\n",
    "    \n",
    "\n",
    "# mask_path = '/media/data_16T/chloe/try/Train/Urban/masks_png/1366.png'\n",
    "# vis_path = '/media/data_16T/chloe/try/Train/Urban/images_png/1366.png'\n",
    "\n",
    "\n",
    "# # segmentation_map = np.array(Image.open(mask_path)).astype(np.uint8)\n",
    "\n",
    "#     #  mask_path = r'C:\\Users\\86158\\Desktop\\Wujin_9_6.png'\n",
    "#     # vis_path = r'C:\\Users\\86158\\Desktop\\Wujin_9_6_vis.png'\n",
    "\n",
    "# render(mask_path, vis_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[255, 255, 255],\n",
       " [255, 0, 0],\n",
       " [255, 255, 0],\n",
       " [0, 0, 255],\n",
       " [159, 129, 183],\n",
       " [0, 255, 0],\n",
       " [255, 195, 128]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def color_palette():\n",
    "    \"\"\"Color palette that maps each class to RGB values.\n",
    "    \n",
    "    This one is actually taken from ADE20k.\n",
    "    \"\"\"\n",
    "    return [[255, 255, 255], [255,0,0], [255,255,0], [0,0,255], [159,129,183], [0,255,0], [255,195, 128]]\n",
    "\n",
    "palette = color_palette()\n",
    "palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colored_mask = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8)\n",
    "# for class_value, color in COLOR_MAP.items():\n",
    "#     class_pixels = np.where(normal_image == class_value)\n",
    "#     colored_mask[class_pixels] = color_palette[class_value]\n",
    "\n",
    "\n",
    "# # cv2.imwrite(\"colored_mask.png\", colored_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGNORE\n",
      "(0, 0, 0)\n",
      "Background\n",
      "(255, 255, 255)\n",
      "Building\n",
      "(255, 0, 0)\n",
      "Road\n",
      "(255, 255, 0)\n",
      "Water\n",
      "(0, 0, 255)\n",
      "Barren\n",
      "(159, 129, 183)\n",
      "Forest\n",
      "(0, 255, 0)\n",
      "Agricultural\n",
      "(255, 195, 128)\n"
     ]
    }
   ],
   "source": [
    "for class_value, color in COLOR_MAP.items():\n",
    "    print(class_value)\n",
    "    print(color)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IGNORE': (0, 0, 0),\n",
       " 'Background': (255, 255, 255),\n",
       " 'Building': (255, 0, 0),\n",
       " 'Road': (255, 255, 0),\n",
       " 'Water': (0, 0, 255),\n",
       " 'Barren': (159, 129, 183),\n",
       " 'Forest': (0, 255, 0),\n",
       " 'Agricultural': (255, 195, 128)}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLOR_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map ={\n",
    "    0: \"Ignore\",\n",
    "    1: \"Background\",\n",
    "    2: \"Building\",\n",
    "    3: \"Road\",\n",
    "    4: \"Water\",\n",
    "    5: \"Barren\",\n",
    "    6: \"Forest\",\n",
    "    7: \"Agricultural\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mapping = {\n",
    "    1: [0,0,0],\n",
    "    2: [255, 255, 255],   # Example mapping for number 1 to red color\n",
    "    3: [255, 255, 0],   # Example mapping for number 2 to green color\n",
    "    4: [0, 0, 255],   # Example mapping for number 3 to blue color\n",
    "    5: [159, 129, 183], \n",
    "    6: [0,255,0],\n",
    "    7: [255,195,128]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_path = \"/media/data_16T/chloe/2516.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       ...,\n",
       "       [2, 2, 2, ..., 1, 1, 1],\n",
       "       [2, 2, 2, ..., 1, 1, 1],\n",
       "       [2, 2, 2, ..., 1, 1, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmentation_map = np.array(Image.open(mask_path)).astype(np.uint8)\n",
    "segmentation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 2-dimensional, but 3 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-39d869092ae3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegmentation_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Assuming number is in the first channel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolor_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Retrieve color from dictionary or default to black\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mcolor_segmentation_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 3 were indexed"
     ]
    }
   ],
   "source": [
    "for i in range(1024):\n",
    "    for j in range(1024):\n",
    "        number = segmentation_map[i, j, 0]  # Assuming number is in the first channel\n",
    "        color = color_mapping.get(number, [0, 0, 0])  # Retrieve color from dictionary or default to black\n",
    "        color_segmentation_map[i, j, :] = color"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping code that works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "# i = 0\n",
    "# while i < 2:\n",
    "#     for i in segmentation_map[i][i]:\n",
    "#         if segmentation_map[i][i][1] == 6:\n",
    "#             color_segmentation_map[:] = [255, 195, 128]\n",
    "#         i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1024, 3)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_segmentation_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the img fed in here has to be a mask \n",
    "img = cv2.imread(\"/media/data_16T/chloe/0.png\") # NOTE: need to copy the entire path, not just the relative path \n",
    "np.array(img)\n",
    "img = cv2.resize(img, (1024,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_segmentation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 61,  62,  69],\n",
       "        [ 62,  66,  70],\n",
       "        [ 65,  69,  72],\n",
       "        ...,\n",
       "        [ 87,  80,  70],\n",
       "        [ 95,  85,  74],\n",
       "        [101,  88,  78]],\n",
       "\n",
       "       [[ 59,  61,  70],\n",
       "        [ 60,  68,  72],\n",
       "        [ 60,  67,  68],\n",
       "        ...,\n",
       "        [ 94,  84,  74],\n",
       "        [101,  89,  79],\n",
       "        [106,  94,  82]],\n",
       "\n",
       "       [[ 61,  66,  74],\n",
       "        [ 60,  64,  68],\n",
       "        [ 54,  59,  62],\n",
       "        ...,\n",
       "        [100,  89,  79],\n",
       "        [103,  92,  82],\n",
       "        [104,  92,  82]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 63,  58,  51],\n",
       "        [ 58,  55,  48],\n",
       "        [ 57,  55,  48],\n",
       "        ...,\n",
       "        [ 66,  63,  58],\n",
       "        [ 63,  61,  57],\n",
       "        [ 64,  63,  56]],\n",
       "\n",
       "       [[ 54,  51,  42],\n",
       "        [ 52,  50,  42],\n",
       "        [ 56,  53,  46],\n",
       "        ...,\n",
       "        [ 56,  55,  47],\n",
       "        [ 59,  58,  47],\n",
       "        [ 57,  56,  47]],\n",
       "\n",
       "       [[ 54,  47,  41],\n",
       "        [ 52,  46,  41],\n",
       "        [ 48,  44,  39],\n",
       "        ...,\n",
       "        [ 55,  55,  47],\n",
       "        [ 59,  58,  47],\n",
       "        [ 55,  55,  45]]], dtype=uint8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_image = \"/media/data_16T/chloe/2518.png\"\n",
    "cv2.imread(normal_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U30'), dtype('float64')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-1d9b16f5058e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mground_truth_color_seg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcolored_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mground_truth_color_seg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcolored_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U30'), dtype('float64')) -> None"
     ]
    }
   ],
   "source": [
    "ground_truth_color_seg = color_segmentation_map[..., ::-1] # reverses the colo channel into BGR\n",
    "ground_truth_color_seg\n",
    "\n",
    "colored_mask = np.array(normal_image) * 0.5 + ground_truth_color_seg * 0.5\n",
    "colored_mask = img.astype(np.uint8)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.RandomCrop(width=512, height=512),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[123.675, 116.28, 103.53],\n",
    "                          std=[58.395, 57.12, 57.375])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(width=512,height=512), \n",
    "    transforms.Normalize(mean=[123.675, 116.28, 103.53],\n",
    "                          std=[58.395, 57.12, 57.375])\n",
    "])\n",
    "\n",
    "class LoveDA(Dataset):\n",
    "    def __init__(self, imagePaths, maskPaths, transforms):\n",
    "        self.imagePaths = imagePaths\n",
    "        self.maskPaths = maskPaths \n",
    "        self.transforms = transforms \n",
    "\n",
    "        self.image_files=os.listdir(\"/media/data_16T/chloe/try/Train/Urban/images_png\")\n",
    "        self.mask_files = os.listdir(\"/media/data_16T/chloe/try/Train/Urban/masks_png\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imagePaths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        imagePath = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        maskPath = os.path.join(self.mask_dir, self.mask_files[idx])\n",
    "\n",
    "        image = Image.open(imagePath).convert(\"RGB\")\n",
    "        mask = Image.open(maskPath).convert(\"L\")\n",
    "\n",
    "   \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "            mask = self.transforms(mask)\n",
    "        \n",
    "        return (image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LoveDA(train_image_dir, train_mask_dir, transform=transform)\n",
    "val_dataset = LoveDA(val_image_dir, val_mask_dir, transform=transform)\n",
    "test_dataset = LoveDA(test_image_dir, test_mask_dir, transform=transform)\n",
    "\n",
    "\n",
    "train_image_dir = \"/media/data_16T/chloe/try/Train/Urban/images_png\"\n",
    "train_mask_dir = \"/media/data_16T/chloe/try/Train/Urban/masks_png\"\n",
    "val_image_dir = \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
