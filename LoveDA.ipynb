{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chia/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# July 2nd Next Steps: Debug the transforms image problem, fine-tune Mask2Former on LoveDA dataset and figure out how to save \n",
    "\n",
    "import torch \n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from torch.utils.data import Dataset \n",
    "from datasets import load_dataset\n",
    "import cv2\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os \n",
    "import torchvision as torchvision\n",
    "from torch.utils.data import Dataset \n",
    "import torchvision.transforms as transforms\n",
    "import torchvision as torchvision\n",
    "import albumentations as A\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Ignore', 1: 'Background', 2: 'Building', 3: 'Road', 4: 'Water', 5: 'Barren', 6: 'Forest', 7: 'Agricultural'}\n"
     ]
    }
   ],
   "source": [
    "repo_id = f\"chloechia/loveda\"\n",
    "filename = \"id2label.json\"\n",
    "id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\"), \"r\"))\n",
    "id2label = {int(k):v for k,v in id2label.items()}\n",
    "print(id2label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagefolder (/home/chia/.cache/huggingface/datasets/chloechia___imagefolder/chloechia--correctloveda-701d9bc57e72e64b/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n",
      "100%|██████████| 2/2 [00:00<00:00,  9.92it/s]\n",
      "Found cached dataset imagefolder (/home/chia/.cache/huggingface/datasets/chloechia___imagefolder/chloechia--correctloveda-701d9bc57e72e64b/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n",
      "Found cached dataset imagefolder (/home/chia/.cache/huggingface/datasets/chloechia___imagefolder/chloechia--correctloveda-701d9bc57e72e64b/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n",
      "Found cached dataset imagefolder (/home/chia/.cache/huggingface/datasets/chloechia___imagefolder/chloechia--maskloveda-30d1338528c17a6a/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n",
      "100%|██████████| 2/2 [00:00<00:00, 41.93it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"chloechia/correctloveda\")\n",
    "train_image = load_dataset(\"chloechia/correctloveda\", split=\"train\")\n",
    "validation_image = load_dataset(\"chloechia/correctloveda\", split=\"validation\")\n",
    "mask_dataset = load_dataset(\"chloechia/maskloveda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagefolder (/home/chia/.cache/huggingface/datasets/chloechia___imagefolder/chloechia--maskloveda-30d1338528c17a6a/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n",
      "Found cached dataset imagefolder (/home/chia/.cache/huggingface/datasets/chloechia___imagefolder/chloechia--maskloveda-30d1338528c17a6a/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    }
   ],
   "source": [
    "train_image = train_image.remove_columns(\"label\")\n",
    "validation_image = validation_image.remove_columns(\"label\")\n",
    "train_mask_dataset = load_dataset(\"chloechia/maskloveda\", split=\"train\")\n",
    "train_mask_dataset = train_mask_dataset.remove_columns(\"label\")\n",
    "val_mask_dataset = load_dataset(\"chloechia/maskloveda\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagefolder (/home/chia/.cache/huggingface/datasets/chloechia___imagefolder/chloechia--maskloveda-30d1338528c17a6a/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n"
     ]
    }
   ],
   "source": [
    "val_mask_dataset = load_dataset(\"chloechia/maskloveda\", split=\"validation\")\n",
    "val_mask_dataset = val_mask_dataset.remove_columns(\"label\")\n",
    "val_mask_dataset = val_mask_dataset.rename_column(\"image\", \"mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask_dataset = train_mask_dataset.rename_column(\"image\", \"mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['image', 'mask'],\n",
       "     num_rows: 1366\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['image', 'mask'],\n",
       "     num_rows: 1669\n",
       " }))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "official_train = concatenate_datasets([train_image, train_mask_dataset], axis=1)\n",
    "official_val = concatenate_datasets([validation_image, val_mask_dataset], axis=1)\n",
    "official_train, official_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n"
     ]
    }
   ],
   "source": [
    "masky = official_train[0]['image']\n",
    "image_shape = masky.size\n",
    "print(type(masky))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "num_channels = len(masky.mode)\n",
    "print(num_channels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# train_transform expects an img. train_transform(img)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(90),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    \n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(90),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    \n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    \n",
    "    \n",
    "])\n",
    "\n",
    "mask_test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    \n",
    "])\n",
    "\n",
    "# train_transform.transforms = list(train_transform.transforms)\n",
    "# mask_transform.transforms = list(mask_transform.transforms)\n",
    "# test_transform.transforms = list(test_transform.transforms)\n",
    "# mask_test_transform.transforms = list(mask_test_transform.transforms)\n",
    "\n",
    "# train_transform.transforms.append(transforms.Lambda(lambda x: x * 0.5 + 0.5))\n",
    "# mask_transform.transforms.append(transforms.Lambda(lambda x: x * 0.5 + 0.5))\n",
    "# test_transform.transforms.append(transforms.Lambda(lambda x: x * 0.5 + 0.5))\n",
    "# mask_test_transform.transforms.append(transforms.Lambda(lambda x: x * 0.5 + 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSegmentationDataset(Dataset):\n",
    "    \"\"\"Image segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, transform, mask_transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        original_image = self.dataset[idx]['image']\n",
    "        original_segmentation_map = self.dataset[idx]['mask']\n",
    "        image = self.transform(original_image)\n",
    "        mask = self.mask_transform(original_segmentation_map)\n",
    "\n",
    "\n",
    "        return image, mask, original_image, original_segmentation_map\n",
    "\n",
    "\n",
    "train_dataset = ImageSegmentationDataset(dataset=official_train, transform=train_transform, mask_transform=mask_transform)\n",
    "test_dataset = ImageSegmentationDataset(dataset=official_val, transform=test_transform, mask_transform=mask_test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1024, 1024])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, mask, original_image, original_segmentation_map = train_dataset[0]\n",
    "tensors = [image, mask]\n",
    "scaled_tensors = [(t - torch.min(t)) / (torch.max(t) - torch.min(t)) for t in tensors]\n",
    "image, mask = scaled_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Background', 'Building', 'Road', 'Water', 'Agricultural']\n"
     ]
    }
   ],
   "source": [
    "labels = [id2label[label] for label in np.unique(segmentation_map)]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[255, 255, 255],\n",
       " [255, 0, 0],\n",
       " [255, 255, 0],\n",
       " [0, 0, 255],\n",
       " [159, 129, 183],\n",
       " [0, 255, 0],\n",
       " [255, 195, 128]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def color_palette():\n",
    "    return [[255, 255, 255], [255,0,0], [255,255,0], [0,0,255], [159,129,183], [0,255,0], [255,195, 128]]\n",
    "\n",
    "palette = color_palette()\n",
    "palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Mask2FormerImageProcessor, Mask2FormerConfig, Mask2FormerForUniversalSegmentation, AutoImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2 as cv2 \n",
    "#processor = Mask2FormerImageProcessor.from_pretrained(\"facebook/mask2former-swin-small-cityscapes-semantic\")\n",
    "#model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-small-cityscapes-semantic\")\n",
    "\n",
    "CONFIG = \"facebook/mask2former-swin-large-cityscapes-semantic\"\n",
    "processor = Mask2FormerImageProcessor.from_pretrained(CONFIG)\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(CONFIG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the last Fully Connected Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=256, out_features=7, bias=True)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn \n",
    "fc = nn.Linear(in_features=256, out_features=7)\n",
    "model.class_predictor = fc\n",
    "model.class_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100, 256])\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(original_image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(outputs.transformer_decoder_last_hidden_state.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(batch) \u001b[39m# gives you the raw pixel values of the image (but some values are negative) \u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(batch[\u001b[39m\"\u001b[39m\u001b[39mmask_labels\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m# mask labels and class labels are different? \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate():\n",
    "    print(batch) # gives you the raw pixel values of the image (but some values are negative) \n",
    "    print(batch[\"mask_labels\"]) # mask labels and class labels are different? \n",
    "    print(batch[\"class_labels\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# function where we use the preprocessor of MaskFormer to turn the images and maps into the format that MaskFormer expects \n",
    "# per-pixel annotations of the segmentation map will be turned into a set of binary masks and corresponding labels \n",
    "def collate_fn(batch):\n",
    "    inputs = list(zip(*batch))\n",
    "    images = inputs[0]\n",
    "    segmentation_maps = inputs[1]\n",
    "    # this function pads the inputs to the same size,\n",
    "    # and creates a pixel mask\n",
    "    # actually padding isn't required here since we are cropping\n",
    "    batch = processor(\n",
    "        images,\n",
    "        segmentation_maps=segmentation_maps,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    batch[\"original_images\"] = inputs[2]\n",
    "    batch[\"original_segmentation_maps\"] = inputs[3]\n",
    "    \n",
    "    return batch\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/683 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The image to be converted to a PIL image contains values outside the range [0, 1], got [-2.1179039478302, 2.1804091930389404] which cannot be converted to uint8.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch:\u001b[39m\u001b[39m\"\u001b[39m, epoch)\n\u001b[1;32m     13\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfor\u001b[39;00m idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(train_dataloader)): \u001b[39m# this iterates through all the batches. one for loop = one batch is put through the training loop. train_dataloader is loading in the batches\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[39m# Reset the parameter gradients\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m     \u001b[39m# Forward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "Cell \u001b[0;32mIn[129], line 12\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      8\u001b[0m segmentation_maps \u001b[39m=\u001b[39m inputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[39m# this function pads the inputs to the same size,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# and creates a pixel mask\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m# actually padding isn't required here since we are cropping\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m batch \u001b[39m=\u001b[39m processor(\n\u001b[1;32m     13\u001b[0m     images,\n\u001b[1;32m     14\u001b[0m     segmentation_maps\u001b[39m=\u001b[39;49msegmentation_maps,\n\u001b[1;32m     15\u001b[0m     return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m batch[\u001b[39m\"\u001b[39m\u001b[39moriginal_images\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m inputs[\u001b[39m2\u001b[39m]\n\u001b[1;32m     19\u001b[0m batch[\u001b[39m\"\u001b[39m\u001b[39moriginal_segmentation_maps\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m inputs[\u001b[39m3\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/transformers/models/mask2former/image_processing_mask2former.py:542\u001b[0m, in \u001b[0;36mMask2FormerImageProcessor.__call__\u001b[0;34m(self, images, segmentation_maps, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images, segmentation_maps\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchFeature:\n\u001b[0;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(images, segmentation_maps\u001b[39m=\u001b[39;49msegmentation_maps, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/transformers/models/mask2former/image_processing_mask2former.py:696\u001b[0m, in \u001b[0;36mMask2FormerImageProcessor.preprocess\u001b[0;34m(self, images, segmentation_maps, instance_id_to_semantic_id, do_resize, size, size_divisor, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, ignore_index, reduce_labels, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[39mif\u001b[39;00m segmentation_maps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(images) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(segmentation_maps):\n\u001b[1;32m    694\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImages and segmentation maps must have the same length.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 696\u001b[0m images \u001b[39m=\u001b[39m [\n\u001b[1;32m    697\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_image(\n\u001b[1;32m    698\u001b[0m         image,\n\u001b[1;32m    699\u001b[0m         do_resize\u001b[39m=\u001b[39mdo_resize,\n\u001b[1;32m    700\u001b[0m         size\u001b[39m=\u001b[39msize,\n\u001b[1;32m    701\u001b[0m         size_divisor\u001b[39m=\u001b[39msize_divisor,\n\u001b[1;32m    702\u001b[0m         resample\u001b[39m=\u001b[39mresample,\n\u001b[1;32m    703\u001b[0m         do_rescale\u001b[39m=\u001b[39mdo_rescale,\n\u001b[1;32m    704\u001b[0m         rescale_factor\u001b[39m=\u001b[39mrescale_factor,\n\u001b[1;32m    705\u001b[0m         do_normalize\u001b[39m=\u001b[39mdo_normalize,\n\u001b[1;32m    706\u001b[0m         image_mean\u001b[39m=\u001b[39mimage_mean,\n\u001b[1;32m    707\u001b[0m         image_std\u001b[39m=\u001b[39mimage_std,\n\u001b[1;32m    708\u001b[0m         data_format\u001b[39m=\u001b[39mdata_format,\n\u001b[1;32m    709\u001b[0m     )\n\u001b[1;32m    710\u001b[0m     \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images\n\u001b[1;32m    711\u001b[0m ]\n\u001b[1;32m    713\u001b[0m \u001b[39mif\u001b[39;00m segmentation_maps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    714\u001b[0m     segmentation_maps \u001b[39m=\u001b[39m [\n\u001b[1;32m    715\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_mask(segmentation_map, do_resize, size, size_divisor)\n\u001b[1;32m    716\u001b[0m         \u001b[39mfor\u001b[39;00m segmentation_map \u001b[39min\u001b[39;00m segmentation_maps\n\u001b[1;32m    717\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/transformers/models/mask2former/image_processing_mask2former.py:697\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[39mif\u001b[39;00m segmentation_maps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(images) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(segmentation_maps):\n\u001b[1;32m    694\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImages and segmentation maps must have the same length.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    696\u001b[0m images \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 697\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess_image(\n\u001b[1;32m    698\u001b[0m         image,\n\u001b[1;32m    699\u001b[0m         do_resize\u001b[39m=\u001b[39;49mdo_resize,\n\u001b[1;32m    700\u001b[0m         size\u001b[39m=\u001b[39;49msize,\n\u001b[1;32m    701\u001b[0m         size_divisor\u001b[39m=\u001b[39;49msize_divisor,\n\u001b[1;32m    702\u001b[0m         resample\u001b[39m=\u001b[39;49mresample,\n\u001b[1;32m    703\u001b[0m         do_rescale\u001b[39m=\u001b[39;49mdo_rescale,\n\u001b[1;32m    704\u001b[0m         rescale_factor\u001b[39m=\u001b[39;49mrescale_factor,\n\u001b[1;32m    705\u001b[0m         do_normalize\u001b[39m=\u001b[39;49mdo_normalize,\n\u001b[1;32m    706\u001b[0m         image_mean\u001b[39m=\u001b[39;49mimage_mean,\n\u001b[1;32m    707\u001b[0m         image_std\u001b[39m=\u001b[39;49mimage_std,\n\u001b[1;32m    708\u001b[0m         data_format\u001b[39m=\u001b[39;49mdata_format,\n\u001b[1;32m    709\u001b[0m     )\n\u001b[1;32m    710\u001b[0m     \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images\n\u001b[1;32m    711\u001b[0m ]\n\u001b[1;32m    713\u001b[0m \u001b[39mif\u001b[39;00m segmentation_maps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    714\u001b[0m     segmentation_maps \u001b[39m=\u001b[39m [\n\u001b[1;32m    715\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_mask(segmentation_map, do_resize, size, size_divisor)\n\u001b[1;32m    716\u001b[0m         \u001b[39mfor\u001b[39;00m segmentation_map \u001b[39min\u001b[39;00m segmentation_maps\n\u001b[1;32m    717\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/transformers/models/mask2former/image_processing_mask2former.py:582\u001b[0m, in \u001b[0;36mMask2FormerImageProcessor._preprocess_image\u001b[0;34m(self, image, do_resize, size, size_divisor, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, data_format)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[39m# All transformations expect numpy arrays.\u001b[39;00m\n\u001b[1;32m    581\u001b[0m image \u001b[39m=\u001b[39m to_numpy_array(image)\n\u001b[0;32m--> 582\u001b[0m image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess(\n\u001b[1;32m    583\u001b[0m     image\u001b[39m=\u001b[39;49mimage,\n\u001b[1;32m    584\u001b[0m     do_resize\u001b[39m=\u001b[39;49mdo_resize,\n\u001b[1;32m    585\u001b[0m     size\u001b[39m=\u001b[39;49msize,\n\u001b[1;32m    586\u001b[0m     size_divisor\u001b[39m=\u001b[39;49msize_divisor,\n\u001b[1;32m    587\u001b[0m     resample\u001b[39m=\u001b[39;49mresample,\n\u001b[1;32m    588\u001b[0m     do_rescale\u001b[39m=\u001b[39;49mdo_rescale,\n\u001b[1;32m    589\u001b[0m     rescale_factor\u001b[39m=\u001b[39;49mrescale_factor,\n\u001b[1;32m    590\u001b[0m     do_normalize\u001b[39m=\u001b[39;49mdo_normalize,\n\u001b[1;32m    591\u001b[0m     image_mean\u001b[39m=\u001b[39;49mimage_mean,\n\u001b[1;32m    592\u001b[0m     image_std\u001b[39m=\u001b[39;49mimage_std,\n\u001b[1;32m    593\u001b[0m )\n\u001b[1;32m    594\u001b[0m \u001b[39mif\u001b[39;00m data_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    595\u001b[0m     image \u001b[39m=\u001b[39m to_channel_dimension_format(image, data_format)\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/transformers/models/mask2former/image_processing_mask2former.py:558\u001b[0m, in \u001b[0;36mMask2FormerImageProcessor._preprocess\u001b[0;34m(self, image, do_resize, size, size_divisor, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_preprocess\u001b[39m(\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    546\u001b[0m     image: ImageInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    555\u001b[0m     image_std: Optional[Union[\u001b[39mfloat\u001b[39m, List[\u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    556\u001b[0m ):\n\u001b[1;32m    557\u001b[0m     \u001b[39mif\u001b[39;00m do_resize:\n\u001b[0;32m--> 558\u001b[0m         image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresize(image, size\u001b[39m=\u001b[39;49msize, size_divisor\u001b[39m=\u001b[39;49msize_divisor, resample\u001b[39m=\u001b[39;49mresample)\n\u001b[1;32m    559\u001b[0m     \u001b[39mif\u001b[39;00m do_rescale:\n\u001b[1;32m    560\u001b[0m         image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescale(image, rescale_factor\u001b[39m=\u001b[39mrescale_factor)\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/transformers/models/mask2former/image_processing_mask2former.py:502\u001b[0m, in \u001b[0;36mMask2FormerImageProcessor.resize\u001b[0;34m(self, image, size, size_divisor, resample, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    492\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSize must contain \u001b[39m\u001b[39m'\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m\u001b[39m keys or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mshortest_edge\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlongest_edge\u001b[39m\u001b[39m'\u001b[39m\u001b[39m keys. Got\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    493\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m size \u001b[39m=\u001b[39m get_mask2former_resize_output_image_size(\n\u001b[1;32m    496\u001b[0m     image\u001b[39m=\u001b[39mimage,\n\u001b[1;32m    497\u001b[0m     size\u001b[39m=\u001b[39msize,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    500\u001b[0m     default_to_square\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    501\u001b[0m )\n\u001b[0;32m--> 502\u001b[0m image \u001b[39m=\u001b[39m resize(image, size\u001b[39m=\u001b[39;49msize, resample\u001b[39m=\u001b[39;49mresample, data_format\u001b[39m=\u001b[39;49mdata_format)\n\u001b[1;32m    503\u001b[0m \u001b[39mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/transformers/image_transforms.py:306\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy)\u001b[0m\n\u001b[1;32m    304\u001b[0m do_rescale \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(image, PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mImage):\n\u001b[0;32m--> 306\u001b[0m     do_rescale \u001b[39m=\u001b[39m _rescale_for_pil_conversion(image)\n\u001b[1;32m    307\u001b[0m     image \u001b[39m=\u001b[39m to_pil_image(image, do_rescale\u001b[39m=\u001b[39mdo_rescale)\n\u001b[1;32m    308\u001b[0m height, width \u001b[39m=\u001b[39m size\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/transformers/image_transforms.py:141\u001b[0m, in \u001b[0;36m_rescale_for_pil_conversion\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    139\u001b[0m     do_rescale \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    142\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe image to be converted to a PIL image contains values outside the range [0, 1], \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgot [\u001b[39m\u001b[39m{\u001b[39;00mimage\u001b[39m.\u001b[39mmin()\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mimage\u001b[39m.\u001b[39mmax()\u001b[39m}\u001b[39;00m\u001b[39m] which cannot be converted to uint8.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    145\u001b[0m \u001b[39mreturn\u001b[39;00m do_rescale\n",
      "\u001b[0;31mValueError\u001b[0m: The image to be converted to a PIL image contains values outside the range [0, 1], got [-2.1179039478302, 2.1804091930389404] which cannot be converted to uint8."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "running_loss = 0.0\n",
    "num_samples = 0\n",
    "for epoch in range(20):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  model.train()\n",
    "  for idx, batch in enumerate(tqdm(train_dataloader)): # this iterates through all the batches. one for loop = one batch is put through the training loop. train_dataloader is loading in the batches\n",
    "      # Reset the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(\n",
    "          pixel_values=batch[\"pixel_values\"].to(device),\n",
    "          mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "          class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "      )\n",
    "\n",
    "      # Backward propagation\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "\n",
    "      batch_size = batch[\"pixel_values\"].size(0)\n",
    "      running_loss += loss.item()\n",
    "      num_samples += batch_size\n",
    "\n",
    "      if idx % 100 == 0:\n",
    "        print(\"Loss:\", running_loss/num_samples)\n",
    "\n",
    "      # Optimization\n",
    "      optimizer.step()\n",
    "\n",
    "  model.eval()\n",
    "  for idx, batch in enumerate(tqdm(test_dataloader)):\n",
    "    if idx > 5:\n",
    "      break\n",
    "\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "      outputs = model(pixel_values=pixel_values.to(device))\n",
    "\n",
    "    # get original images\n",
    "    original_images = batch[\"original_images\"]\n",
    "    target_sizes = [(image.shape[0], image.shape[1]) for image in original_images]\n",
    "    # predict segmentation maps\n",
    "    predicted_segmentation_maps = processor.post_process_semantic_segmentation(outputs,\n",
    "                                                                                  target_sizes=target_sizes)\n",
    "\n",
    "    # get ground truth segmentation maps\n",
    "    ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n",
    "\n",
    "    metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "  \n",
    "  # NOTE this metric outputs a dict that also includes the mIoU per category as keys\n",
    "  # so if you're interested, feel free to prcint them as well\n",
    "  print(\"Mean IoU:\", metric.compute(num_labels = len(id2label), ignore_index = 0)['mean_iou'])\n",
    "\n",
    "\n",
    "# torch.save(model.state_dict(), 'model.pt')\n",
    "# # with model.eval(), you're not updating the weights. \n",
    "# # {\n",
    "#     'layer1.weight': tensor([...]),    # Example weight tensor of 'layer1'\n",
    "#     'layer1.bias': tensor([...]),      # Example bias tensor of 'layer1'\n",
    "#     'layer2.weight': tensor([...]),    # Example weight tensor of 'layer2'\n",
    "#     'layer2.bias': tensor([...]),      # Example bias tensor of 'layer2'\n",
    "#     ...\n",
    "# }\n",
    "# Each parameter in the model has a corresponding entry in the state dictionary. In the example above, 'layer1.weight' corresponds to the weight tensor of the first layer, 'layer1.bias' corresponds to the bias tensor of the first layer, and so on. The ellipsis ([...]) represents the values of the tensors.\n",
    "\n",
    "# When you load the model.pt file using model.load_state_dict(torch.load('model.pt')), the state dictionary is loaded into the model, and the model's parameters are updated with the saved tensor values, restoring the model to the state it was in when it was saved.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The image to be converted to a PIL image contains values outside the range [0, 1], got [-2.1179039478302, 2.640000104904175] which cannot be converted to uint8.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(batch) \u001b[39m# gives you the raw pixel values of the image (but some values are negative) \u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(batch[\u001b[39m\"\u001b[39m\u001b[39mmask_labels\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m# mask labels and class labels are different? \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "Cell \u001b[0;32mIn[129], line 12\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      8\u001b[0m segmentation_maps \u001b[39m=\u001b[39m inputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[39m# this function pads the inputs to the same size,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# and creates a pixel mask\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m# actually padding isn't required here since we are cropping\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m batch \u001b[39m=\u001b[39m processor(\n\u001b[1;32m     13\u001b[0m     images,\n\u001b[1;32m     14\u001b[0m     segmentation_maps\u001b[39m=\u001b[39;49msegmentation_maps,\n\u001b[1;32m     15\u001b[0m     return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m batch[\u001b[39m\"\u001b[39m\u001b[39moriginal_images\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m inputs[\u001b[39m2\u001b[39m]\n\u001b[1;32m     19\u001b[0m batch[\u001b[39m\"\u001b[39m\u001b[39moriginal_segmentation_maps\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m inputs[\u001b[39m3\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/transformers/models/mask2former/image_processing_mask2former.py:542\u001b[0m, in \u001b[0;36mMask2FormerImageProcessor.__call__\u001b[0;34m(self, images, segmentation_maps, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images, segmentation_maps\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchFeature:\n\u001b[0;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(images, segmentation_maps\u001b[39m=\u001b[39;49msegmentation_maps, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/transformers/models/mask2former/image_processing_mask2former.py:696\u001b[0m, in \u001b[0;36mMask2FormerImageProcessor.preprocess\u001b[0;34m(self, images, segmentation_maps, instance_id_to_semantic_id, do_resize, size, size_divisor, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, ignore_index, reduce_labels, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[39mif\u001b[39;00m segmentation_maps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(images) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(segmentation_maps):\n\u001b[1;32m    694\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImages and segmentation maps must have the same length.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 696\u001b[0m images \u001b[39m=\u001b[39m [\n\u001b[1;32m    697\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_image(\n\u001b[1;32m    698\u001b[0m         image,\n\u001b[1;32m    699\u001b[0m         do_resize\u001b[39m=\u001b[39mdo_resize,\n\u001b[1;32m    700\u001b[0m         size\u001b[39m=\u001b[39msize,\n\u001b[1;32m    701\u001b[0m         size_divisor\u001b[39m=\u001b[39msize_divisor,\n\u001b[1;32m    702\u001b[0m         resample\u001b[39m=\u001b[39mresample,\n\u001b[1;32m    703\u001b[0m         do_rescale\u001b[39m=\u001b[39mdo_rescale,\n\u001b[1;32m    704\u001b[0m         rescale_factor\u001b[39m=\u001b[39mrescale_factor,\n\u001b[1;32m    705\u001b[0m         do_normalize\u001b[39m=\u001b[39mdo_normalize,\n\u001b[1;32m    706\u001b[0m         image_mean\u001b[39m=\u001b[39mimage_mean,\n\u001b[1;32m    707\u001b[0m         image_std\u001b[39m=\u001b[39mimage_std,\n\u001b[1;32m    708\u001b[0m         data_format\u001b[39m=\u001b[39mdata_format,\n\u001b[1;32m    709\u001b[0m     )\n\u001b[1;32m    710\u001b[0m     \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images\n\u001b[1;32m    711\u001b[0m ]\n\u001b[1;32m    713\u001b[0m \u001b[39mif\u001b[39;00m segmentation_maps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    714\u001b[0m     segmentation_maps \u001b[39m=\u001b[39m [\n\u001b[1;32m    715\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_mask(segmentation_map, do_resize, size, size_divisor)\n\u001b[1;32m    716\u001b[0m         \u001b[39mfor\u001b[39;00m segmentation_map \u001b[39min\u001b[39;00m segmentation_maps\n\u001b[1;32m    717\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/transformers/models/mask2former/image_processing_mask2former.py:697\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[39mif\u001b[39;00m segmentation_maps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(images) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(segmentation_maps):\n\u001b[1;32m    694\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImages and segmentation maps must have the same length.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    696\u001b[0m images \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 697\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess_image(\n\u001b[1;32m    698\u001b[0m         image,\n\u001b[1;32m    699\u001b[0m         do_resize\u001b[39m=\u001b[39;49mdo_resize,\n\u001b[1;32m    700\u001b[0m         size\u001b[39m=\u001b[39;49msize,\n\u001b[1;32m    701\u001b[0m         size_divisor\u001b[39m=\u001b[39;49msize_divisor,\n\u001b[1;32m    702\u001b[0m         resample\u001b[39m=\u001b[39;49mresample,\n\u001b[1;32m    703\u001b[0m         do_rescale\u001b[39m=\u001b[39;49mdo_rescale,\n\u001b[1;32m    704\u001b[0m         rescale_factor\u001b[39m=\u001b[39;49mrescale_factor,\n\u001b[1;32m    705\u001b[0m         do_normalize\u001b[39m=\u001b[39;49mdo_normalize,\n\u001b[1;32m    706\u001b[0m         image_mean\u001b[39m=\u001b[39;49mimage_mean,\n\u001b[1;32m    707\u001b[0m         image_std\u001b[39m=\u001b[39;49mimage_std,\n\u001b[1;32m    708\u001b[0m         data_format\u001b[39m=\u001b[39;49mdata_format,\n\u001b[1;32m    709\u001b[0m     )\n\u001b[1;32m    710\u001b[0m     \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images\n\u001b[1;32m    711\u001b[0m ]\n\u001b[1;32m    713\u001b[0m \u001b[39mif\u001b[39;00m segmentation_maps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    714\u001b[0m     segmentation_maps \u001b[39m=\u001b[39m [\n\u001b[1;32m    715\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_mask(segmentation_map, do_resize, size, size_divisor)\n\u001b[1;32m    716\u001b[0m         \u001b[39mfor\u001b[39;00m segmentation_map \u001b[39min\u001b[39;00m segmentation_maps\n\u001b[1;32m    717\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/transformers/models/mask2former/image_processing_mask2former.py:582\u001b[0m, in \u001b[0;36mMask2FormerImageProcessor._preprocess_image\u001b[0;34m(self, image, do_resize, size, size_divisor, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, data_format)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[39m# All transformations expect numpy arrays.\u001b[39;00m\n\u001b[1;32m    581\u001b[0m image \u001b[39m=\u001b[39m to_numpy_array(image)\n\u001b[0;32m--> 582\u001b[0m image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess(\n\u001b[1;32m    583\u001b[0m     image\u001b[39m=\u001b[39;49mimage,\n\u001b[1;32m    584\u001b[0m     do_resize\u001b[39m=\u001b[39;49mdo_resize,\n\u001b[1;32m    585\u001b[0m     size\u001b[39m=\u001b[39;49msize,\n\u001b[1;32m    586\u001b[0m     size_divisor\u001b[39m=\u001b[39;49msize_divisor,\n\u001b[1;32m    587\u001b[0m     resample\u001b[39m=\u001b[39;49mresample,\n\u001b[1;32m    588\u001b[0m     do_rescale\u001b[39m=\u001b[39;49mdo_rescale,\n\u001b[1;32m    589\u001b[0m     rescale_factor\u001b[39m=\u001b[39;49mrescale_factor,\n\u001b[1;32m    590\u001b[0m     do_normalize\u001b[39m=\u001b[39;49mdo_normalize,\n\u001b[1;32m    591\u001b[0m     image_mean\u001b[39m=\u001b[39;49mimage_mean,\n\u001b[1;32m    592\u001b[0m     image_std\u001b[39m=\u001b[39;49mimage_std,\n\u001b[1;32m    593\u001b[0m )\n\u001b[1;32m    594\u001b[0m \u001b[39mif\u001b[39;00m data_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    595\u001b[0m     image \u001b[39m=\u001b[39m to_channel_dimension_format(image, data_format)\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/transformers/models/mask2former/image_processing_mask2former.py:558\u001b[0m, in \u001b[0;36mMask2FormerImageProcessor._preprocess\u001b[0;34m(self, image, do_resize, size, size_divisor, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_preprocess\u001b[39m(\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    546\u001b[0m     image: ImageInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    555\u001b[0m     image_std: Optional[Union[\u001b[39mfloat\u001b[39m, List[\u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    556\u001b[0m ):\n\u001b[1;32m    557\u001b[0m     \u001b[39mif\u001b[39;00m do_resize:\n\u001b[0;32m--> 558\u001b[0m         image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresize(image, size\u001b[39m=\u001b[39;49msize, size_divisor\u001b[39m=\u001b[39;49msize_divisor, resample\u001b[39m=\u001b[39;49mresample)\n\u001b[1;32m    559\u001b[0m     \u001b[39mif\u001b[39;00m do_rescale:\n\u001b[1;32m    560\u001b[0m         image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescale(image, rescale_factor\u001b[39m=\u001b[39mrescale_factor)\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/transformers/models/mask2former/image_processing_mask2former.py:502\u001b[0m, in \u001b[0;36mMask2FormerImageProcessor.resize\u001b[0;34m(self, image, size, size_divisor, resample, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    492\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSize must contain \u001b[39m\u001b[39m'\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m\u001b[39m keys or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mshortest_edge\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlongest_edge\u001b[39m\u001b[39m'\u001b[39m\u001b[39m keys. Got\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    493\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m size \u001b[39m=\u001b[39m get_mask2former_resize_output_image_size(\n\u001b[1;32m    496\u001b[0m     image\u001b[39m=\u001b[39mimage,\n\u001b[1;32m    497\u001b[0m     size\u001b[39m=\u001b[39msize,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    500\u001b[0m     default_to_square\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    501\u001b[0m )\n\u001b[0;32m--> 502\u001b[0m image \u001b[39m=\u001b[39m resize(image, size\u001b[39m=\u001b[39;49msize, resample\u001b[39m=\u001b[39;49mresample, data_format\u001b[39m=\u001b[39;49mdata_format)\n\u001b[1;32m    503\u001b[0m \u001b[39mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/transformers/image_transforms.py:306\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy)\u001b[0m\n\u001b[1;32m    304\u001b[0m do_rescale \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(image, PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mImage):\n\u001b[0;32m--> 306\u001b[0m     do_rescale \u001b[39m=\u001b[39m _rescale_for_pil_conversion(image)\n\u001b[1;32m    307\u001b[0m     image \u001b[39m=\u001b[39m to_pil_image(image, do_rescale\u001b[39m=\u001b[39mdo_rescale)\n\u001b[1;32m    308\u001b[0m height, width \u001b[39m=\u001b[39m size\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/transformers/image_transforms.py:141\u001b[0m, in \u001b[0;36m_rescale_for_pil_conversion\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    139\u001b[0m     do_rescale \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    142\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe image to be converted to a PIL image contains values outside the range [0, 1], \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgot [\u001b[39m\u001b[39m{\u001b[39;00mimage\u001b[39m.\u001b[39mmin()\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mimage\u001b[39m.\u001b[39mmax()\u001b[39m}\u001b[39;00m\u001b[39m] which cannot be converted to uint8.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    145\u001b[0m \u001b[39mreturn\u001b[39;00m do_rescale\n",
      "\u001b[0;31mValueError\u001b[0m: The image to be converted to a PIL image contains values outside the range [0, 1], got [-2.1179039478302, 2.640000104904175] which cannot be converted to uint8."
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(train_dataloader):\n",
    "    print(batch) # gives you the raw pixel values of the image (but some values are negative) \n",
    "    print(batch[\"mask_labels\"]) # mask labels and class labels are different? \n",
    "    print(batch[\"class_labels\"])\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Palette\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (70, 70, 70), 1: (128, 64, 128), 2: (152, 251, 152), 3: (107, 142, 35)}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "# building, road, barren, vegetation \n",
    "newidtocolor: Dict[int, Tuple[int, int, int]] = {\n",
    "    \"0\": (70, 70, 70),\n",
    "    \"1\": (128, 64, 128),\n",
    "    \"2\": (152, 251, 152),\n",
    "    \"3\":(107, 142, 35),\n",
    "}\n",
    "\n",
    "newidtocolor: Dict[int, Tuple[int, int, int]] = {\n",
    "    int(key): value for key, value in newidtocolor.items()\n",
    "}\n",
    "\n",
    "newidtocolor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bizon_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
