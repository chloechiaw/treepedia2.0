{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chia/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# print(model.model.transformer_module.decoder)\n",
    "# print(model.class_predictor)\n",
    "# - Get the linear layer with the class predictor \n",
    "\n",
    "import torch \n",
    "import random\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from torch.utils.data import Dataset \n",
    "from datasets import load_dataset\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os \n",
    "import torchvision as torchvision\n",
    "from torch.utils.data import Dataset \n",
    "import torchvision.transforms as transforms\n",
    "import torchvision as torchvision\n",
    "import albumentations as A\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeTransform(object):\n",
    "    def __call__(self, img):\n",
    "        img = img / 255.0\n",
    "        return img\n",
    "    \n",
    "train_image_dir = '/media/data_16T/chloe/try/Dataset/urbanruraltrainimages'\n",
    "train_mask_dir = '/media/data_16T/chloe/try/Dataset/urbanruraltrainmasks'\n",
    "val_image_dir = '/media/data_16T/chloe/try/Dataset/urbanruralvalimages'\n",
    "val_mask_dir = '/media/data_16T/chloe/try/Dataset/urbanruralvalmasks'\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# train_transform expects an img. train_transform(img)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(90),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "    #                      std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "])\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    # transforms.RandomRotation(90),\n",
    "    # transforms.RandomVerticalFlip(p=0.5),\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    NormalizeTransform()\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "    #                      std=[0.229, 0.224, 0.225])\n",
    "\n",
    "])\n",
    "\n",
    "image_test_transform = transforms.Compose([\n",
    "    NormalizeTransform()\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Ignore', 1: 'Background', 2: 'Building', 3: 'Road', 4: 'Water', 5: 'Barren', 6: 'Forest', 7: 'Agricultural'}\n"
     ]
    }
   ],
   "source": [
    "repo_id = f\"chloechia/loveda\"\n",
    "filename = \"id2label.json\"\n",
    "id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\"), \"r\"))\n",
    "id2label = {int(k):v for k,v in id2label.items()}\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-large-cityscapes-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.weight: found shape torch.Size([20, 256]) in the checkpoint and torch.Size([9, 256]) in the model instantiated\n",
      "- class_predictor.bias: found shape torch.Size([20]) in the checkpoint and torch.Size([9]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([20]) in the checkpoint and torch.Size([9]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import cv2 as cv2 \n",
    "from transformers import Mask2FormerImageProcessor, Mask2FormerConfig, Mask2FormerForUniversalSegmentation, AutoImageProcessor\n",
    "\n",
    "CONFIG = \"facebook/mask2former-swin-large-cityscapes-semantic\"\n",
    "processor = Mask2FormerImageProcessor(ignore_index=0,reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(CONFIG, id2label=id2label, ignore_mismatched_sizes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# class ApplyTransforms(object):\n",
    "#     def __init__(self, transforms):\n",
    "#         self.transforms = transforms\n",
    "\n",
    "#     def __call__(self, image, mask):\n",
    "#         # Apply the same random transformations to both image and mask\n",
    "#         seed = torch.randint(0, 1000000, (1,)).item()\n",
    "#         random_transforms = transforms.RandomApply(self.transforms, p=1.0)\n",
    "#         transformed_image = random_transforms(image, seed)\n",
    "#         transformed_mask = random_transforms(mask, seed)\n",
    "\n",
    "#         return transformed_image, transformed_mask\n",
    "        \n",
    "class CustomSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, image_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.image_transform = image_transform\n",
    "        self.image_files = os.listdir(image_dir)\n",
    "        self.mask_files = os.listdir(mask_dir)\n",
    "        self.num_files = len(self.image_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_files\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[index])\n",
    "        \n",
    "        original_image = Image.open(image_path).convert('RGB')\n",
    "        original_mask = Image.open(mask_path)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            seed = np.random.randint(10000)\n",
    "            random.seed(seed)\n",
    "            image = self.transform(original_image)\n",
    "            random.seed(seed)\n",
    "            mask = self.transform(original_mask)\n",
    "            # image = self.transform(original_image)\n",
    "            # mask = self.transform(original_mask)\n",
    "            mask = np.squeeze(mask)\n",
    "            \n",
    "        if self.image_transform is not None:\n",
    "            image = self.image_transform(image)\n",
    "\n",
    "        # return image, mask, original_image, original_mask\n",
    "        return np.array(image), np.array(mask), np.array(original_image), np.array(original_mask)\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    inputs = list(zip(*batch))\n",
    "    images = inputs[0]\n",
    "    segmentation_maps = inputs[1]\n",
    "    \n",
    "    batch = processor(\n",
    "        images,\n",
    "        segmentation_maps=segmentation_maps,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    batch[\"original_images\"] = inputs[2]\n",
    "    batch[\"original_segmentation_maps\"] = inputs[3]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomSegmentationDataset(train_image_dir, train_mask_dir, transform=train_transform, image_transform=image_transform)\n",
    "val_dataset = CustomSegmentationDataset(val_image_dir, val_mask_dir, transform=test_transform, image_transform=image_test_transform)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([1, 3, 1024, 1024])\n",
      "pixel_mask torch.Size([1, 1024, 1024])\n",
      "mask_labels torch.Size([5, 1024, 1024])\n",
      "class_labels torch.Size([5])\n",
      "original_images (1024, 1024, 3)\n",
      "original_segmentation_maps (1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k,v.shape)\n",
    "  else:\n",
    "    print(k,v[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.class_predictor.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn \n",
    "# model.class_predictor.weight = nn.Parameter(model.class_predictor.weight[:8, :])\n",
    "# model.class_predictor.bias = nn.Parameter(model.class_predictor.bias[:8])\n",
    "# model.criterion.empty_weight = nn.Parameter(model.criterion.empty_weight[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.criterion.empty_weight.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0],\n",
       " [255, 255, 255],\n",
       " [255, 0, 0],\n",
       " [255, 255, 0],\n",
       " [0, 0, 255],\n",
       " [159, 129, 183],\n",
       " [0, 255, 0],\n",
       " [255, 195, 128]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def color_palette():\n",
    "    return [[0,0,0], [255, 255, 255], [255,0,0], [255,255,0], [0,0,255], [159,129,183], [0,255,0], [255,195, 128]]\n",
    "\n",
    "palette = color_palette()\n",
    "palette"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the last Fully Connected Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn \n",
    "# fc = nn.Linear(in_features=256, out_features=8)\n",
    "# model.class_predictor = fc\n",
    "# model.class_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn \n",
    "# model.class_predictor.weight = nn.Parameter(model.class_predictor.weight[:8, :])\n",
    "# model.class_predictor.bias = nn.Parameter(model.class_predictor.bias[:8])\n",
    "# model.criterion.empty_weight = nn.Parameter(model.criterion.empty_weight[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.class_predictor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Background', 'Road', 'Water', 'Forest', 'Agricultural']\n"
     ]
    }
   ],
   "source": [
    "# Verify the labels\n",
    "labels = [id2label[label] for label in batch[\"class_labels\"][0].tolist()]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.init.xavier_uniform_(model.class_predictor.weight).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([105.2779], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(batch[\"pixel_values\"],\n",
    "                class_labels=batch[\"class_labels\"],\n",
    "                mask_labels=batch[\"mask_labels\"])\n",
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1366 [00:16<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 22.00 MiB (GPU 1; 10.76 GiB total capacity; 9.52 GiB already allocated; 6.31 MiB free; 9.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m outputs \u001b[39m=\u001b[39m model(\n\u001b[1;32m     20\u001b[0m     pixel_values\u001b[39m=\u001b[39;49mbatch[\u001b[39m\"\u001b[39;49m\u001b[39mpixel_values\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device),\n\u001b[1;32m     21\u001b[0m     mask_labels\u001b[39m=\u001b[39;49m[labels\u001b[39m.\u001b[39;49mto(device) \u001b[39mfor\u001b[39;49;00m labels \u001b[39min\u001b[39;49;00m batch[\u001b[39m\"\u001b[39;49m\u001b[39mmask_labels\u001b[39;49m\u001b[39m\"\u001b[39;49m]],\n\u001b[1;32m     22\u001b[0m     class_labels\u001b[39m=\u001b[39;49m[labels\u001b[39m.\u001b[39;49mto(device) \u001b[39mfor\u001b[39;49;00m labels \u001b[39min\u001b[39;49;00m batch[\u001b[39m\"\u001b[39;49m\u001b[39mclass_labels\u001b[39;49m\u001b[39m\"\u001b[39;49m]],\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[39m# Backward propagation\u001b[39;00m\n\u001b[1;32m     26\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/transformers/models/mask2former/modeling_mask2former.py:2500\u001b[0m, in \u001b[0;36mMask2FormerForUniversalSegmentation.forward\u001b[0;34m(self, pixel_values, mask_labels, class_labels, pixel_mask, output_hidden_states, output_auxiliary_logits, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m   2495\u001b[0m output_hidden_states \u001b[39m=\u001b[39m (\n\u001b[1;32m   2496\u001b[0m     output_hidden_states \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39moutput_hidden_states\n\u001b[1;32m   2497\u001b[0m )\n\u001b[1;32m   2498\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 2500\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   2501\u001b[0m     pixel_values\u001b[39m=\u001b[39;49mpixel_values,\n\u001b[1;32m   2502\u001b[0m     pixel_mask\u001b[39m=\u001b[39;49mpixel_mask,\n\u001b[1;32m   2503\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49muse_auxiliary_loss,\n\u001b[1;32m   2504\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2505\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2506\u001b[0m )\n\u001b[1;32m   2508\u001b[0m loss, loss_dict, auxiliary_logits \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2509\u001b[0m class_queries_logits \u001b[39m=\u001b[39m ()\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/transformers/models/mask2former/modeling_mask2former.py:2271\u001b[0m, in \u001b[0;36mMask2FormerModel.forward\u001b[0;34m(self, pixel_values, pixel_mask, output_hidden_states, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m   2268\u001b[0m \u001b[39mif\u001b[39;00m pixel_mask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2269\u001b[0m     pixel_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((batch_size, height, width), device\u001b[39m=\u001b[39mpixel_values\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 2271\u001b[0m pixel_level_module_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpixel_level_module(\n\u001b[1;32m   2272\u001b[0m     pixel_values\u001b[39m=\u001b[39;49mpixel_values, output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states\n\u001b[1;32m   2273\u001b[0m )\n\u001b[1;32m   2275\u001b[0m transformer_module_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_module(\n\u001b[1;32m   2276\u001b[0m     multi_scale_features\u001b[39m=\u001b[39mpixel_level_module_output\u001b[39m.\u001b[39mdecoder_hidden_states,\n\u001b[1;32m   2277\u001b[0m     mask_features\u001b[39m=\u001b[39mpixel_level_module_output\u001b[39m.\u001b[39mdecoder_last_hidden_state,\n\u001b[1;32m   2278\u001b[0m     output_hidden_states\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   2279\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   2280\u001b[0m )\n\u001b[1;32m   2282\u001b[0m encoder_hidden_states \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/transformers/models/mask2former/modeling_mask2former.py:1400\u001b[0m, in \u001b[0;36mMask2FormerPixelLevelModule.forward\u001b[0;34m(self, pixel_values, output_hidden_states)\u001b[0m\n\u001b[1;32m   1398\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, pixel_values: Tensor, output_hidden_states: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Mask2FormerPixelLevelModuleOutput:\n\u001b[1;32m   1399\u001b[0m     backbone_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(pixel_values)\u001b[39m.\u001b[39mfeature_maps\n\u001b[0;32m-> 1400\u001b[0m     decoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(backbone_features, output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states)\n\u001b[1;32m   1402\u001b[0m     \u001b[39mreturn\u001b[39;00m Mask2FormerPixelLevelModuleOutput(\n\u001b[1;32m   1403\u001b[0m         encoder_last_hidden_state\u001b[39m=\u001b[39mbackbone_features[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\n\u001b[1;32m   1404\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39m\u001b[39mtuple\u001b[39m(backbone_features) \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1405\u001b[0m         decoder_last_hidden_state\u001b[39m=\u001b[39mdecoder_output\u001b[39m.\u001b[39mmask_features,\n\u001b[1;32m   1406\u001b[0m         decoder_hidden_states\u001b[39m=\u001b[39mdecoder_output\u001b[39m.\u001b[39mmulti_scale_features,\n\u001b[1;32m   1407\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/bizon_py39/lib/python3.9/site-packages/transformers/models/mask2former/modeling_mask2former.py:1314\u001b[0m, in \u001b[0;36mMask2FormerPixelDecoder.forward\u001b[0;34m(self, features, encoder_outputs, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1312\u001b[0m position_embeddings \u001b[39m=\u001b[39m [embed\u001b[39m.\u001b[39mflatten(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m) \u001b[39mfor\u001b[39;00m embed \u001b[39min\u001b[39;00m position_embeddings]\n\u001b[1;32m   1313\u001b[0m level_pos_embed_flat \u001b[39m=\u001b[39m [x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlevel_embed[i]\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m i, x \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(position_embeddings)]\n\u001b[0;32m-> 1314\u001b[0m level_pos_embed_flat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat(level_pos_embed_flat, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m   1316\u001b[0m level_start_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((spatial_shapes\u001b[39m.\u001b[39mnew_zeros((\u001b[39m1\u001b[39m,)), spatial_shapes\u001b[39m.\u001b[39mprod(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcumsum(\u001b[39m0\u001b[39m)[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]))\n\u001b[1;32m   1317\u001b[0m valid_ratios \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_valid_ratio(mask) \u001b[39mfor\u001b[39;00m mask \u001b[39min\u001b[39;00m masks], \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 1; 10.76 GiB total capacity; 9.52 GiB already allocated; 6.31 MiB free; 9.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "running_loss = 0.0\n",
    "num_samples = 0\n",
    "for epoch in range(5):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  model.train()\n",
    "  for idx, batch in enumerate(tqdm(train_dataloader)): # this iterates through all the batches. one for loop = one batch is put through the training loop. train_dataloader is loading in the batches\n",
    "      # Reset the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(\n",
    "          pixel_values=batch[\"pixel_values\"].to(device),\n",
    "          mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "          class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "      )\n",
    "\n",
    "      # Backward propagation\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "\n",
    "      batch_size = batch[\"pixel_values\"].size(0)\n",
    "      running_loss += loss.item()\n",
    "      num_samples += batch_size\n",
    "\n",
    "      if idx % 100 == 0:\n",
    "        print(\"Loss:\", running_loss/num_samples)\n",
    "\n",
    "      # Optimization\n",
    "      optimizer.step()\n",
    "\n",
    "  model.eval()\n",
    "  for idx, batch in enumerate(tqdm(val_dataloader)):\n",
    "    if idx > 5:\n",
    "      break\n",
    "\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "      outputs = model(pixel_values=pixel_values.to(device))\n",
    "\n",
    "    # get original images\n",
    "    original_images = batch[\"original_images\"]\n",
    "    target_sizes = [(image.shape[0], image.shape[1]) for image in original_images]\n",
    "    # predict segmentation maps\n",
    "    predicted_segmentation_maps = processor.post_process_semantic_segmentation(outputs,\n",
    "                                                                                  target_sizes=target_sizes)\n",
    "\n",
    "    # get ground truth segmentation maps\n",
    "    ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n",
    "\n",
    "    metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "  \n",
    "  # NOTE this metric outputs a dict that also includes the mIoU per category as keys\n",
    "  # so if you're interested, feel free to prcint them as well\n",
    "  print(\"Mean IoU:\", metric.compute(num_labels = len(id2label), ignore_index = 0)['mean_iou'])\n",
    "\n",
    "# torch.save(model.state_dict(), 'model.pt')\n",
    "# # with model.eval(), you're not updating the weights. \n",
    "# # {\n",
    "#     'layer1.weight': tensor([...]),    # Example weight tensor of 'layer1'\n",
    "#     'layer1.bias': tensor([...]),      # Example bias tensor of 'layer1'\n",
    "#     'layer2.weight': tensor([...]),    # Example weight tensor of 'layer2'\n",
    "#     'layer2.bias': tensor([...]),      # Example bias tensor of 'layer2'\n",
    "#     ...\n",
    "# }\n",
    "# Each parameter in the model has a corresponding entry in the state dictionary. In the example above, 'layer1.weight' corresponds to the weight tensor of the first layer, 'layer1.bias' corresponds to the bias tensor of the first layer, and so on. The ellipsis ([...]) represents the values of the tensors.\n",
    "\n",
    "# When you load the model.pt file using model.load_state_dict(torch.load('model.pt')), the state dictionary is loaded into the model, and the model's parameters are updated with the saved tensor values, restoring the model to the state it was in when it was saved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Palette\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (70, 70, 70), 1: (128, 64, 128), 2: (152, 251, 152), 3: (107, 142, 35)}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "# building, road, barren, vegetation \n",
    "newidtocolor: Dict[int, Tuple[int, int, int]] = {\n",
    "    \"0\": (70, 70, 70),\n",
    "    \"1\": (128, 64, 128),\n",
    "    \"2\": (152, 251, 152),\n",
    "    \"3\":(107, 142, 35),\n",
    "}\n",
    "\n",
    "newidtocolor: Dict[int, Tuple[int, int, int]] = {\n",
    "    int(key): value for key, value in newidtocolor.items()\n",
    "}\n",
    "\n",
    "newidtocolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bizon_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
